from trl import SFTTrainer, SFTConfig
from transformers import AutoTokenizer, TrainerCallback
from dataloader import load_data
from model_new import Qwen3MoleculeLLM
import torch
import os
from typing import Dict, List, Any
import logging
from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# è‡ªå®šä¹‰å›è°ƒå‡½æ•°ï¼Œç”¨äºç›‘æ§è®­ç»ƒè¿‡ç¨‹
class LoraTrainingMonitorCallback(TrainerCallback):
    """LoRAè®­ç»ƒç›‘æ§å›è°ƒå‡½æ•°"""
    
    def on_log(self, args, state, control, logs=None, **kwargs):
        """è®°å½•è®­ç»ƒæ—¥å¿—"""
        if logs:
            if 'loss' in logs:
                logger.info(f"Step {state.global_step}: loss = {logs['loss']:.4f}")
            if 'learning_rate' in logs:
                logger.info(f"Step {state.global_step}: learning rate = {logs['learning_rate']:.6f}")
    
    def on_train_begin(self, args, state, control, **kwargs):
        """è®­ç»ƒå¼€å§‹æ—¶è®°å½•LoRAå‚æ•°ä¿¡æ¯"""
        if 'model' in kwargs:
            model = kwargs['model']
            # æ‰“å°å¯è®­ç»ƒå‚æ•°ä¿¡æ¯
            total_params = sum(p.numel() for p in model.parameters())
            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
            logger.info(f"LoRAæ¨¡å‹å‚æ•°ç»Ÿè®¡:")
            logger.info(f"  æ€»å‚æ•°: {total_params:,}")
            logger.info(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
            logger.info(f"  å¯è®­ç»ƒæ¯”ä¾‹: {100 * trainable_params / total_params:.2f}%")
            
            # æ‰“å°LoRAé€‚é…å™¨ä¿¡æ¯
            if hasattr(model, 'peft_config'):
                for adapter_name, config in model.peft_config.items():
                    logger.info(f"  LoRAé…ç½® - {adapter_name}:")
                    logger.info(f"    r={config.r}, alpha={config.lora_alpha}, dropout={config.lora_dropout}")

# å†…å­˜ä¼˜åŒ–çš„collate_fn
def collate_fn(
    batch,
    smiles_len=10*5,           # 4 smiles + 2 special tokens
    pad_token_id=0,
    label_pad_id=-100,
):
    max_len = max(len(x["input_ids"]) for x in batch)

    input_ids = []
    attention_mask = []
    labels = []

    for x in batch:
        ids = x["input_ids"]
        mask = x["attention_mask"]
        lab = x["labels"]

        pad_len = max_len - len(ids)

        # text
        input_ids.append(ids + [pad_token_id] * pad_len)
        attention_mask.append(mask + [label_pad_id] * pad_len)

        # ğŸš¨ å…³é”®ï¼šlabels å¯¹é½ logits
        labels.append(
            [label_pad_id] * smiles_len +   # smiles + special tokens
            lab  +                         # answer labels
            [label_pad_id] * pad_len         # padding
        )
        print("input_id_len",len(input_ids))

    return {
        "input_ids": torch.tensor(input_ids, dtype=torch.long),
        "attention_mask": torch.tensor(attention_mask, dtype=torch.long),
        "labels": torch.tensor(labels, dtype=torch.long),
        "smiles": [[x["smiles"].replace(".", "")] for x in batch],
    }


# LoRA SFTè®­ç»ƒå‡½æ•°
def train_sft_lora(
    model_name="/zengdaojian/zhangjia/BioLatent/Qwen8B",
    data_path="/zengdaojian/zhangjia/BioLatent/ChemCotDataset/chemcotbench-cot",
    output_dir="./qwen3_mol_sft_lora_results",
    epochs=3,
    batch_size=32,  # LoRAå¯ä»¥ä½¿ç”¨ç¨å¤§çš„æ‰¹æ¬¡
    lr=2e-4,
    max_seq_length=512,
):
    """
    ä½¿ç”¨LoRAè¿›è¡ŒSFTè®­ç»ƒï¼Œæ˜¾è‘—å‡å°‘å†…å­˜ä½¿ç”¨
    """
    logger.info("=" * 60)
    logger.info("LoRA SFT Training for Qwen3MoleculeLLM")
    logger.info("=" * 60)
    
    # ============================
    # 1. åˆå§‹åŒ–åŸºç¡€æ¨¡å‹
    # ============================
    logger.info("Initializing base model...")
    model = Qwen3MoleculeLLM(qwen_model_name=model_name)
    tokenizer = model.tokenizer
    
    # ç¡®ä¿pad_tokenè®¾ç½®
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        logger.info(f"Set pad_token to eos_token: {tokenizer.pad_token}")
    
    # å†»ç»“æ‰€æœ‰å‚æ•°ï¼ˆåªè®­ç»ƒLoRAé€‚é…å™¨ï¼‰
    logger.info("Freezing base model parameters...")
    for param in model.parameters():
        param.requires_grad = False
    
    # ============================
    # 2. é…ç½®LoRA
    # ============================
    logger.info("Configuring LoRA...")
    
    # LoRAé…ç½®
    lora_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        r=16,  # LoRAç§©
        lora_alpha=32,  # LoRA alpha
        lora_dropout=0.1,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],  # Qwençš„æ¨¡å—
        bias="none",
    )
    
    # å°†LoRAé€‚é…å™¨æ·»åŠ åˆ°LLMéƒ¨åˆ†
    logger.info("Adding LoRA adapters to LLM...")
    model.model = get_peft_model(model.model, lora_config)
    
    # ç¡®ä¿æŠ•å½±å™¨å¯è®­ç»ƒ
    logger.info("Keeping projector trainable...")
    for param in model.projector.parameters():
        param.requires_grad = True
    
    # ============================
    # 3. åŠ è½½æ•°æ®é›†
    # ============================
    logger.info(f"Loading dataset from {data_path}...")
    train_dataset = load_data(data_path)
    logger.info(f"Dataset loaded: {len(train_dataset)} samples")
    
    # ============================
    # 4. é…ç½®è®­ç»ƒå‚æ•°
    # ============================
    logger.info("Configuring training arguments...")
    
    training_args = SFTConfig(
        output_dir=output_dir,
        num_train_epochs=epochs,
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=1,
        gradient_accumulation_steps=4,  # æ¢¯åº¦ç´¯ç§¯
        learning_rate=lr,
        bf16=True,  # ä½¿ç”¨bfloat16
        max_seq_length=max_seq_length,
        packing=False,
        dataset_text_field=None,
        remove_unused_columns=False,
        logging_steps=10,
        save_steps=100,
        save_total_limit=3,
        gradient_checkpointing=False,  # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
        max_grad_norm=0.3,  # æ¢¯åº¦è£å‰ª
        warmup_ratio=0.1,
        report_to=[],  # ç¦ç”¨wandb/tensorboard
        dataloader_pin_memory=False,
        dataloader_num_workers=2,
        optim="adamw_8bit",  # 8-bitä¼˜åŒ–å™¨èŠ‚çœå†…å­˜
    )
    
    # æ‰“å°è®­ç»ƒé…ç½®
    logger.info(f"Training Configuration:")
    logger.info(f"  Model: {model_name}")
    logger.info(f"  Epochs: {epochs}")
    logger.info(f"  Batch size: {batch_size}")
    logger.info(f"  Learning rate: {lr}")
    logger.info(f"  Max sequence length: {max_seq_length}")
    logger.info(f"  LoRA r: {lora_config.r}")
    logger.info(f"  LoRA alpha: {lora_config.lora_alpha}")
    
    # ============================
    # 5. åˆå§‹åŒ–SFTTrainer
    # ============================
    logger.info("Initializing SFTTrainer...")
    
    
    trainer = SFTTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        tokenizer=tokenizer,
        data_collator=collate_fn,
        callbacks=[LoraTrainingMonitorCallback()],
    )
    
    # ============================
    # 6. è®­ç»ƒå‰éªŒè¯
    # ============================
    logger.info("Testing forward pass...")
    try:
        # æµ‹è¯•ä¸€ä¸ªæ ·æœ¬
        if len(train_dataset) > 0:
            test_sample = [train_dataset[0]]
            test_batch = collate_fn(test_sample)
            
            # ç§»åŠ¨åˆ°GPU
            if torch.cuda.is_available():
                device = torch.device("cuda")
                model = model.to(device)
                test_batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v 
                            for k, v in test_batch.items()}
            
            # å‰å‘ä¼ æ’­æµ‹è¯•
            with torch.no_grad():
                outputs = model(
                    input_ids=test_batch["input_ids"],
                    attention_mask=test_batch["attention_mask"],
                    labels=test_batch["labels"],
                    smiles=test_batch["smiles"]
                )
            
            logger.info(f"Forward test successful!")
            logger.info(f"  Loss: {outputs.loss.item() if outputs.loss is not None else 'N/A'}")
            
            # æ¸…ç†æµ‹è¯•æ•°æ®
            del test_sample, test_batch, outputs
            torch.cuda.empty_cache() if torch.cuda.is_available() else None
            
    except Exception as e:
        logger.error(f"Forward test failed: {e}")
        raise
    
    # ============================
    # 7. è®­ç»ƒ
    # ============================
    logger.info("Starting LoRA training...")
    
    try:
        trainer.train()
        
        logger.info("LoRA training completed!")
        
        # ============================
        # 8. ä¿å­˜æ¨¡å‹
        # ============================
        logger.info("Saving models...")
        
        # ä¿å­˜å®Œæ•´çš„LoRAæ¨¡å‹ï¼ˆåŒ…æ‹¬åŸºç¡€æ¨¡å‹ï¼‰
        trainer.save_model(output_dir)
        
        # å•ç‹¬ä¿å­˜LoRAé€‚é…å™¨æƒé‡
        lora_weights_path = os.path.join(output_dir, "lora_weights")
        os.makedirs(lora_weights_path, exist_ok=True)
        model.model.save_pretrained(lora_weights_path)
        logger.info(f"LoRA weights saved to: {lora_weights_path}")
        
        # ä¿å­˜æŠ•å½±å™¨æƒé‡
        projector_path = os.path.join(output_dir, "projector.pt")
        torch.save(model.projector.state_dict(), projector_path)
        logger.info(f"Projector weights saved to: {projector_path}")
        
        # ä¿å­˜åˆ†è¯å™¨
        tokenizer.save_pretrained(output_dir)
        
        # ============================
        # 9. åˆå¹¶LoRAæƒé‡ï¼ˆå¯é€‰ï¼‰
        # ============================
        logger.info("Merging LoRA weights with base model...")
        try:
            # åˆå¹¶LoRAæƒé‡åˆ°åŸºç¡€æ¨¡å‹
            merged_model = model.model.merge_and_unload()
            
            # åˆ›å»ºå®Œæ•´æ¨¡å‹ï¼ˆåŒ…å«åˆå¹¶çš„LoRAæƒé‡ï¼‰
            merged_model_path = os.path.join(output_dir, "merged_model")
            os.makedirs(merged_model_path, exist_ok=True)
            
            # ä¿å­˜åˆå¹¶åçš„æ¨¡å‹
            merged_model.save_pretrained(merged_model_path)
            tokenizer.save_pretrained(merged_model_path)
            
            # ä¿å­˜æŠ•å½±å™¨é…ç½®
            torch.save(model.projector.state_dict(), os.path.join(merged_model_path, "projector.pt"))
            
            logger.info(f"Merged model saved to: {merged_model_path}")
            
        except Exception as e:
            logger.warning(f"Failed to merge LoRA weights: {e}")
            logger.warning("Using unmerged model for inference")
        
        return model
        
    except torch.cuda.OutOfMemoryError:
        logger.error("CUDA out of memory during LoRA training!")
        logger.error("Try reducing batch_size or max_seq_length")
        raise
    
    except Exception as e:
        logger.error(f"LoRA training failed: {e}")
        raise


# åŠ è½½LoRAæ¨¡å‹è¿›è¡Œæ¨ç† - ä¿®å¤è®¾å¤‡é—®é¢˜
def load_lora_model_for_inference(
    base_model_path="/zengdaojian/zhangjia/BioLatent/Qwen8B",
    lora_weights_path="./qwen3_mol_sft_lora_results/lora_weights",
    projector_path="./qwen3_mol_sft_lora_results/projector.pt",
    merge_lora=True,
    device="cuda" if torch.cuda.is_available() else "cpu"
):
    """
    åŠ è½½LoRAå¾®è°ƒçš„æ¨¡å‹è¿›è¡Œæ¨ç† - ä¿®å¤è®¾å¤‡ä¸€è‡´æ€§
    """
    logger.info(f"Loading LoRA model for inference on {device}...")
    
    # 1. åŠ è½½åŸºç¡€æ¨¡å‹
    model = Qwen3MoleculeLLM(qwen_model_name=base_model_path)
    tokenizer = model.tokenizer
    
    # 2. ç¡®ä¿pad_tokenè®¾ç½®
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # 3. å°†æ¨¡å‹ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
    model = model.to(device)
    
    # 4. åŠ è½½LoRAæƒé‡
    from peft import PeftModel
    model.model = PeftModel.from_pretrained(model.model, lora_weights_path)
    
    # 5. åˆå¹¶LoRAæƒé‡ï¼ˆå¯é€‰ï¼Œç”¨äºæ›´å¿«æ¨ç†ï¼‰
    if merge_lora:
        logger.info("Merging LoRA weights for faster inference...")
        model.model = model.model.merge_and_unload()
    
    # 6. åŠ è½½æŠ•å½±å™¨æƒé‡å¹¶ç¡®ä¿åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
    if os.path.exists(projector_path):
        # åŠ è½½æ—¶æŒ‡å®šmap_location
        projector_state_dict = torch.load(projector_path, map_location=device)
        model.projector.load_state_dict(projector_state_dict)
        # ç¡®ä¿æŠ•å½±å™¨åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        model.projector = model.projector.to(device)
        logger.info(f"Loaded projector weights to {device} from: {projector_path}")
    
    # 7. ç¡®ä¿æ¨¡å‹å„éƒ¨åˆ†éƒ½åœ¨åŒä¸€è®¾å¤‡ä¸Š
    model = model.to(device)
    
    # 8. æ£€æŸ¥è®¾å¤‡ä¸€è‡´æ€§
    model_devices = set()
    for name, param in model.named_parameters():
        model_devices.add(str(param.device))
    
    if len(model_devices) > 1:
        logger.warning(f"Model parameters are on multiple devices: {model_devices}")
        # å¼ºåˆ¶ç»Ÿä¸€è®¾å¤‡
        model = model.to(device)
    
    logger.info(f"Model loaded successfully on {device}")
    
    # 9. è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    model.eval()
    
    return model, tokenizer


# æ¨ç†æµ‹è¯•å‡½æ•° - ä¿®å¤è®¾å¤‡é—®é¢˜
def test_lora_inference(
    model,
    tokenizer,
    test_smiles=[["CC1[NH2+]CCC1C(=O)Nc1cc(C(N)=O)ccc1Cl"]],
    test_prompts=["Modify the molecule CC1[NH2+]CCC1C(=O)Nc1cc(C(N)=O)ccc1Cl by adding a carboxyl."],
    max_new_tokens=200,
    temperature=0.7,
    top_p=0.9,
    device="cuda" if torch.cuda.is_available() else "cpu"
):
    """
    æµ‹è¯•LoRAæ¨¡å‹çš„æ¨ç†èƒ½åŠ› - ä¿®å¤è®¾å¤‡é—®é¢˜ç‰ˆæœ¬
    """
    logger.info(f"Testing LoRA model inference on {device}...")
    
    # ç¡®ä¿æ¨¡å‹åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
    model.eval()
    model = model.to(device)
    
    results = []
    
    for smiles, prompt in zip(test_smiles, test_prompts):
        print(smiles)
        # æ¸…ç†SMILES
        cleaned_smiles = [smile.replace(".", "").strip() for smile in smiles]
        
        logger.info(f"\n{'='*50}")
        logger.info(f"Input SMILES: {cleaned_smiles}")
        logger.info(f"Input prompt: {prompt}")
        
        try:
            # ç¼–ç æç¤ºæ–‡æœ¬
            encodings = tokenizer(
                prompt,
                padding=True,
                truncation=True,
                max_length=256,
                return_tensors="pt"
            )
            
            # ç¡®ä¿æ‰€æœ‰è¾“å…¥åœ¨ç›¸åŒè®¾å¤‡ä¸Š
            input_ids = encodings["input_ids"].to(device)
            attention_mask = encodings["attention_mask"].to(device)
            
            # æ‰“å°è®¾å¤‡ä¿¡æ¯ä»¥è°ƒè¯•
            logger.info(f"Model device: {next(model.parameters()).device}")
            logger.info(f"Input IDs device: {input_ids.device}")
            logger.info(f"Attention mask device: {attention_mask.device}")
            
            # ç”Ÿæˆå›å¤ - å…³é”®ä¿®å¤ï¼šç¡®ä¿smilesä¹Ÿåœ¨æ­£ç¡®è®¾å¤‡ä¸Šå¤„ç†
            with torch.no_grad():
                # è°ƒç”¨æ¨¡å‹çš„generateæ–¹æ³•
                generated_ids = model.generate(
                    smiles_list=[cleaned_smiles],  # SMILESåˆ—è¡¨
                    input_ids=input_ids,  # æ–‡æœ¬è¾“å…¥
                    attention_mask=attention_mask,  # æ³¨æ„åŠ›æ©ç 
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    top_p=top_p,
                    do_sample=True,
                )
            
            # è§£ç ç»“æœ
            generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
            
            # æå–ç”Ÿæˆçš„å›ç­”
            if generated_text.startswith(prompt):
                answer = generated_text[len(prompt):].strip()
            else:
                answer = generated_text
            
            logger.info(f"Generated response: {answer}")
            results.append({
                "smiles": cleaned_smiles,
                "prompt": prompt,
                "response": answer,
                "full_output": generated_text
            })
            
        except Exception as e:
            logger.error(f"Error during inference: {e}")
            import traceback
            logger.error(f"Traceback: {traceback.format_exc()}")
            
            # å°è¯•æ›´ç®€å•çš„æµ‹è¯•
            try:
                logger.info("Trying simpler forward pass...")
                
                # åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•
                with torch.no_grad():
                    # ä½¿ç”¨ä¸€ä¸ªæ›´ç®€å•çš„å‰å‘ä¼ æ’­
                    test_inputs = {
                        "smiles": [cleaned_smiles],
                        "input_ids": torch.tensor([[tokenizer.bos_token_id]], device=device),
                        "attention_mask": torch.tensor([[1]], device=device),
                    }
                    
                    outputs = model(**test_inputs)
                    logger.info(f"Simple forward pass successful!")
                    
                results.append({
                    "smiles": cleaned_smiles,
                    "prompt": prompt,
                    "response": "[Model loaded but generation may have issues]",
                    "note": "Forward pass successful"
                })
                
            except Exception as e2:
                logger.error(f"Simple test also failed: {e2}")
                results.append({
                    "smiles": cleaned_smiles,
                    "prompt": prompt,
                    "error": str(e2)
                })
        
        logger.info(f"{'='*50}\n")
    
    return results


# è°ƒè¯•å‡½æ•°ï¼šæ£€æŸ¥æ¨¡å‹è®¾å¤‡çŠ¶æ€


# ä¸»å‡½æ•°
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="LoRAå¾®è°ƒå¤šæ¨¡æ€åˆ†å­-è¯­è¨€æ¨¡å‹")
    parser.add_argument("--mode", type=str, choices=["train", "inference"], default="train", help="è¿è¡Œæ¨¡å¼")
    parser.add_argument("--model_path", type=str, default="./qwen3_8B_without_rnx_mol_sft_lora_results", help="æ¨¡å‹è·¯å¾„")
    parser.add_argument("--data_path", type=str, default="/zengdaojian/zhangjia/BioLatent/ChemCotDataset/chemcotbench-cot", help="æ•°æ®è·¯å¾„")
    parser.add_argument("--batch_size", type=int, default=2, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--max_seq_length", type=int, default=512, help="æœ€å¤§åºåˆ—é•¿åº¦")
    parser.add_argument("--epochs", type=int, default=3, help="è®­ç»ƒè½®æ•°")
    
    args = parser.parse_args()
    
    if args.mode == "train":
        # è®­ç»ƒæ¨¡å¼
        trained_model = train_sft_lora(
            data_path=args.data_path,
            output_dir=args.model_path,
            batch_size=args.batch_size,
            max_seq_length=args.max_seq_length,
            epochs=args.epochs,
        )
        
        # è®­ç»ƒå®Œæˆåæµ‹è¯•
        logger.info("Testing trained model...")
        test_lora_inference(
            trained_model,
            trained_model.tokenizer,
            test_smiles=[["CC(=O)OC1=CC=CC=C1C(=O)O"]],
            test_prompts=["Please describe the functional groups of this molecule."]
        )
    
    elif args.mode == "inference":
        # æ¨ç†æ¨¡å¼
        model, tokenizer = load_lora_model_for_inference(
            lora_weights_path=os.path.join(args.model_path, "lora_weights"),
            projector_path=os.path.join(args.model_path, "projector.pt")
        )
        
        # æµ‹è¯•æ¨ç†
        test_lora_inference(
            model,
            tokenizer,
            test_smiles=[["CC(=O)OC1=CC=CC=C1C(=O)O"]],
            test_prompts=["Modify the molecule CC1[NH2+]CCC1C(=O)Nc1cc(C(N)=O)ccc1Cl by adding a carboxyl."]
        )